message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
setwd("/Users/dsharpylo/Documents/RStudio/DATA180_Dickinson/DATA180/hw7")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#setwd("/Users/dsharpylo/Documents/RStudio/DATA180_Dickinson/DATA180/hw7")
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
max(news$year)- min(news$year)
# Chunk 5
charVector <- news$headline_text
head(charVector,6)
# Chunk 6
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
# Chunk 7
#make all of the letters in "wordCorpus" lowercase
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
#remove the punctuation in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removePunctuation)
#remove numbers in "wordCorpus"
wordCorpus <- tm_map(wordCorpus, removeNumbers)
#take out the "stop" words, such as "the", "a" and "at"
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
# Chunk 8
#
tdm <- TermDocumentMatrix(charVector)
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts,10)
# Chunk 10
wordCounts1 <- wordCounts[wordCounts>=50]
barplot(wordCounts1,las=2, cex.axis= 0.75)
# Chunk 11
totalWords <- sum(wordCounts)
matchedP <- match(names(wordCounts), posWords, nomatch = 0)
matchedP <- wordCounts[matchedP != 0]
barplot(matchedP[matchedP>=20],las=2,cex.names=0.75)
sum(matchedP)/totalWords
matchedN <- match(names(wordCounts), negWords, nomatch = 0)
matchedN <- wordCounts[matchedN != 0]
barplot(matchedN[matchedN>=20],las=2,cex.names=0.75)
sum(matchedN)/totalWords
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
library(ggplot2)
ggplot(news, aes(x=factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() +theme(axis.text=element_text(size=4,angle=90))
# Chunk 14
library("quanteda")
# Chunk 15
ex <- charVector
ex<- removePunctuation(ex)
ex<- removeNumbers(ex)
ex<- removeWords(ex, stopwords("en"))
x <- termFreq(ex)
sort(x, decreasing = TRUE)[1:20]
# Chunk 16
library(tokenizers)
#tokenize_ngrams(charVector, n=2)
library(ngram)
words <- paste(unlist(ex), collapse =" ")
ng <- ngram(words,n=2)
table <- get.phrasetable(ng)
head(table, 20)
# Chunk 17
newscorpus <- corpus(charVector)
paragraphs <- corpus_reshape(newscorpus)
# Chunk 18
news_dtm <- dfm(paragraphs, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm, c("s","?","?",'thi'))
news_dtm <- dfm_trim(news_dtm, min_termfreq=50)
head(news_dtm,6)
# Chunk 19
library("quanteda.textplots")
textplot_wordcloud(news_dtm,color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
# Not surprised: police, man, new, govt, court, death
# Surprised: warn, nsw, house
# Chunk 20
# install.packages('gsl')
# install.packages('topicmodels')
# library('topicmodels')
# library('tidytext')
# Chunk 21
# news_topics <- convert(news_dtm, to = "topicmodels") # same as tdm earlier
# topic_model <- LDA(news_topics,k=8)
# terms(topic_model,10)
# Chunk 22
# tidy_topics <- tidy(topic_model, matrix = "beta")
# news_top_topics <- tidy_topics %>%
#   group_by(topic) %>%
#   slice_max(beta, n = 10) %>% # cool func, gets the max n for each topic group
#   ungroup() %>% # to get the tibble without group tag
#   arrange(topic, -beta) # sort by topic, beta decreasing
#
# news_top_topics %>%
#   mutate(term = reorder_within(term, beta, topic)) %>% # this hack is to order for facet
#   ggplot(aes(beta, term, fill = factor(topic))) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~topic, scales = "free") + # scales="free" allows x-y scales to be free.
#   scale_y_reordered() # used in combo with reorder_within
# Chunk 23
# tidy_news <- tidy(topic_model, matrix = "gamma")
# top_news <- tidy_news %>%
#   group_by(topicname) %>%
#   slice_max(gamma, n = 10) %>%
#   ungroup() %>%
#   arrange(document, -gamma)
#
# top_news %>%
#   mutate(document = reorder_within(document, gamma, topicname)) %>%
#   ggplot(aes(gamma, document, fill = factor(topicname))) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~topicname, scales = "free") +
#   scale_y_reordered() # takes care of labels
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#setwd("/Users/tu/Desktop/DATA180-Tu/hw7")
#library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
library(dplyr)
n_distinct(news$year)
# Chunk 5
charVector <- news$headline_text
head(charVector)
# Chunk 6
wordVector <- VectorSource(charVector)
class(wordVector); typeof(wordVector); length(wordVector)
wordCorpus <- Corpus(wordVector)
class(wordCorpus); typeof(wordCorpus); length(wordCorpus)
# Chunk 7
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]] #check after trimming
wordVector[1]
# Chunk 8
tdm <- TermDocumentMatrix(wordCorpus)
tdm
# Chunk 9
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
totalWords <- sum(wordCounts)
# Chunk 10
barplot(wordCounts[wordCounts>=50], las=2, cex.names=0.75, xlab = "Word", ylab = "Frequency")
# Chunk 11
#positive
matchedP <- match(names(wordCounts), posWords, nomatch=0)
matchedP <- wordCounts[matchedP != 0]
sum(matchedP)/totalWords
barplot(matchedP[matchedP>=20])
# Chunk 12
#negative
matchedN <- match(names(wordCounts), negWords, nomatch=0)
matchedN <- wordCounts[matchedN != 0]
sum(matchedN)/totalWords
barplot(matchedN[matchedN>=20], cex.names=0.75, cex = 0.75)
# Chunk 13
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
news
# Chunk 14
library("ggplot2")
ggplot(news, aes(x=factor(yearmonth, levels=unique(yearmonth)))) +
geom_bar() +
xlab("Time") +
ylab("Number of articles")+
theme(axis.text=element_text(size=4, angle=90))
# Chunk 15
library("quanteda")
# Chunk 16
mostFreq <- termFreq(charVector, control = list(removePunctuation=TRUE, stopwords =TRUE))
sort(mostFreq, decreasing = TRUE)[1:20]
# Chunk 17
#how to return a pair
library(tokenizers)
tokenize_words(charVector)
tokenize_ngrams(charVector, n=2)
library(ngram)
words <- paste(unlist(charVector), collapse = " ")
ng <- ngram(words, n=2)
head(get.phrasetable(ng), 20)
# Chunk 18
newscorpus <- corpus(charVector)
paras <- corpus_reshape(newscorpus, to = "paragraphs")
# Chunk 19
news_dtm <- dfm(paras, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
news_dtm <- dfm_remove(news_dtm, c("s","?","?",'thi'))
news_dtm <- dfm_trim(news_dtm, min_termfreq=50) # to trim
head(colnames(news_dtm), 6)
# Chunk 20
library("quanteda.textplots")
textplot_wordcloud(news_dtm,adjust=.6)
# Chunk 21
library("topicmodels")
library('tidytext')
# Chunk 22
news_topics <- convert(news_dtm, to = "topicmodels")
topic_model <- LDA(news_topics, method = "VEM", k=8)
terms(topic_model,10)
# Chunk 23
tidy_topics <- tidy(topic_model, matrix = "beta")
tidy_topics
news_top_topics <- tidy_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>% # cool func, gets the max n for each topic group
ungroup() %>% # to get the tibble without group tag
arrange(topic, -beta)
news_top_topics %>%
mutate(term = reorder_within(term, beta, topic)) %>% # this hack is to order for facet
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") + # scales="free" allows x-y scales to be free.
scale_y_reordered()
# Chunk 24
tidy_topics2 <- tidy(topic_model, matrix = "gamma")
tidy_topics2
top_topics <- tidy_topics2 %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(document, -gamma)
top_topics %>%
mutate(document = reorder_within(document, gamma, topic)) %>%
ggplot(aes(gamma, document, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
scale_y_reordered() # takes care of labels
# Chunk 25
mydf <- data.frame(as.matrix(news_topics))
mydf$id <- rownames(mydf)
topic1 <- mydf %>% filter(id == "text408" | id == "text6370" | id == "text8621" | id == "text1074" | id =="text7674")
topic2 <- mydf %>% filter(id == "text8949" | id == "text4894" | id == "text1367" | id == "text4494" | id =="text4902")
topic1 <- subset(topic1, select = -id)
topic2 <-subset(topic2, select = -id)
topic1 <- data.frame(t(topic1))
topic2 <- data.frame(t(topic2))
topic1[rowSums(topic1>0),]
topic2[rowSums(topic2>0),]
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
years = length(unique(news$year))
cat(years, "years")
# Chunk 5
charVector = news$headline_text
print(head(charVector, 6))
# Chunk 6
wordVector = VectorSource(charVector)
wordCorpus = Corpus(wordVector)
# Chunk 7
#function for transformations
transform = function(doc) {
doc = tolower(doc)
doc = removePunctuation(doc)
doc = removeNumbers(doc)
doc = removeWords(doc, stopwords("en"))
return(doc)
}
wordCorpus = tm_map(wordCorpus, content_transformer(transform))
print(content(wordCorpus[[1]]))
# Chunk 8
# A term document matrix is a representation of a text corpus. rows represent words and columns represents documents. Each can represent the frequency of a term in a document
textCorpus = Corpus(VectorSource(charVector))
textCorpus = tm_map(textCorpus, content_transformer(transform))
tdm = TermDocumentMatrix(textCorpus)
# Chunk 9
m = as.matrix(tdm)
wordCounts = rowSums(m)
wordCounts_df = data.frame(word = names(wordCounts), count = wordCounts, row.names = NULL)
top_words = head(wordCounts_df[order(-wordCounts_df$count),], 10)
print(top_words)
# Chunk 10
library(ggplot2)
filtered_words = subset(wordCounts_df, count >= 50)
barplot_data = ggplot(filtered_words, aes(x = reorder(word, -count), y = count)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top Words in Headlines (appear >=50 times)",
x = "Words",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = rel(.75)))
print(barplot_data)
# Chunk 11
library(dplyr)
freq_words_20 <- wordCounts[wordCounts >= 20]
total_unique_words <- length(wordCounts)
positive_words <- intersect(names(freq_words_20), posWords)
percentage_positive <- length(positive_words) / total_unique_words * 100
negative_words <- intersect(names(freq_words_20), negWords)
percentage_negative <- length(negative_words) / total_unique_words * 100
cat("Percentage of Positive Words:", percentage_positive, "%\n")
cat("Percentage of Negative Words:", percentage_negative, "%\n")
barplot(freq_words_20[positive_words], las = 2, cex.names = 0.75, main = "Most Frequent Positive Words (>= 20 times)")
barplot(freq_words_20[negative_words], las = 2, cex.names = 0.75, main = "Most Frequent Negative Words (>= 20 times)")
# Chunk 12
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
# Chunk 13
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)), y = count)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Number of Articles Released by Year and Month",
x = "Year Month",
y = "Number of Articles Released") +
theme(axis.text.x = element_text(size = 4, angle = 90)) +
scale_x_discrete(labels = function(x) format(as.Date(x, "%Y/%m"), "%Y/%m"))
# Chunk 14
library("quanteda")
# Chunk 15
library("quanteda")
my_corpus = corpus(charVector)
# Tokenize the corpus
my_tokens = tokens(my_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, to_lower = TRUE)
# Create a document-feature matrix (dfm)
my_dfm = dfm(my_tokens)
# Get term frequencies
term_freqs = rowSums(my_dfm)
# Combine term frequencies with terms
term_stats = data.frame(term = names(term_freqs), frequency = term_freqs)
# Filter out stopwords, short words, etc.
filtered_term_stats = term_stats %>%
filter(nchar(term) > 1)
# Get the top 20 words
top_words = head(filtered_term_stats[order(-filtered_term_stats$frequency),], 20)
print(top_words)
# Chunk 16
my_corpus = corpus(charVector)
# Tokenize the corpus
my_tokens = tokens(my_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, to_lower = TRUE)
# Create bigrams
my_bigrams_list = tokens_ngrams(my_tokens, n = 2)
my_bigrams = unlist(my_bigrams_list)
# Calculate bigram frequencies
bigram_freqs = table(my_bigrams)
# Convert to a data frame
bigram_stats = data.frame(bigram = names(bigram_freqs), frequency = as.numeric(bigram_freqs))
# Filter out stopwords, short words, etc.
filtered_bigram_stats = bigram_stats %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(nchar(word1) > 1, nchar(word2) > 1)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)), y = count)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Number of Articles Released by Year and Month",
x = "Year Month",
y = "Number of Articles Released") +
theme(axis.text.x = element_text(size = 4, angle = 90)) +
scale_x_discrete(labels = function(x) format(as.Date(x, "%Y/%m"), "%Y/%m"))
library("quanteda")
my_corpus = corpus(charVector)
# Tokenize the corpus
my_tokens = tokens(my_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, to_lower = TRUE)
# Create a document-feature matrix (dfm)
my_dfm = dfm(my_tokens)
# Get term frequencies
term_freqs = rowSums(my_dfm)
# Combine term frequencies with terms
term_stats = data.frame(term = names(term_freqs), frequency = term_freqs)
# Filter out stopwords, short words, etc.
filtered_term_stats = term_stats %>%
filter(nchar(term) > 1)
# Get the top 20 words
top_words = head(filtered_term_stats[order(-filtered_term_stats$frequency),], 20)
print(top_words)
my_corpus = corpus(charVector)
# Tokenize the corpus
my_tokens = tokens(my_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, to_lower = TRUE)
# Create bigrams
my_bigrams_list = tokens_ngrams(my_tokens, n = 2)
my_bigrams = unlist(my_bigrams_list)
# Calculate bigram frequencies
bigram_freqs = table(my_bigrams)
# Convert to a data frame
bigram_stats = data.frame(bigram = names(bigram_freqs), frequency = as.numeric(bigram_freqs))
# Filter out stopwords, short words, etc.
filtered_bigram_stats = bigram_stats %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(nchar(word1) > 1, nchar(word2) > 1)
newscorpus <- corpus(news$headline_text)
newscorpus <- corpus_reshape(newscorpus, to = "paragraphs")
head(newscorpus)
news_tokens <- tokens(news$headline_text, remove_punct = TRUE)
news_dtm <- dfm(news_tokens, stem=TRUE, remove_punct=TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove=c(stopwords("english")))
news_dtm <- dfm_trim(news_dtm, min_termfreq = 50)
print(news_dtm[1:6, ])
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#library(tidyverse)
library(tm)
news<-read.csv("news.csv",header=T)
# Chunk 3
posWords <- scan("positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
news$year <- as.integer(format(as.Date(news$publish_date, format="%Y%m%d"), "%Y"))
news$year <- as.integer(format(as.Date(news$publish_date, format="%Y%m%d"), "%Y"))
charVector <- news$headline_text
cat("First 6 entries in charVector:\n")
cat(charVector[1:6], "\n")
wordVector <- VectorSource(charVector)
wordCorpus <- Corpus(wordVector)
cat("Preview of wordCorpus:\n")
print(wordCorpus[1:2])
library(tm)
wordCorpus <- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus <- tm_map(wordCorpus, removePunctuation)
wordCorpus <- tm_map(wordCorpus, removeNumbers)
wordCorpus <- tm_map(wordCorpus, removeWords, stopwords("en"))
cat("After trimming:\n")
cat(as.character(wordCorpus[[1]]), "\n")
tdm <- TermDocumentMatrix(wordCorpus)
cat("Term-Document Matrix Summary:\n")
print(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
topWords <- head(sort(wordCounts, decreasing = TRUE), 10)
cat("Top 10 most frequent words:\n")
print(topWords)
freqWords <- rowSums(m) >= 50
barplot(wordCounts[freqWords], las = 2, cex.names = 0.75, main = "Words that Showed Up at Least 50 Times", col = "yellow")
axis(1, at = seq_along(wordCounts[freqWords]), labels = names(wordCounts)[freqWords], las = 2, cex.axis = 0.75)
library(tm)
calculate_and_barplot <- function(words, title) {
percentages <- wordCounts[names(wordCounts) %in% words] / sum(wordCounts) * 100
cat(paste("Percentage of", title, "Words:", percentages, "%\n"))
freqWords <- wordCounts[wordCounts >= 20 & names(wordCounts) %in% words]
if (length(freqWords) > 0 && all(is.finite(freqWords))) {
barplot(freqWords, las = 2, cex.names = 0.75, main = paste("Top", title, "Words (>=20 occurrences)"), col = ifelse(title == "Positive", "red", "blue"))
axis(1, at = seq_along(freqWords), labels = names(freqWords), las = 2, cex.axis = 0.75)
} else {
cat("No valid data for creating the barplot of", title, "words.\n")
}
}
calculate_and_barplot(posWords, "Positive")
calculate_and_barplot(negWords, "Negative")
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
library(dplyr)
library(ggplot2)
news <- news %>%
group_by(year, month) %>%
mutate(count = n(), yearmonth = paste(year, month, sep = '/')) %>%
arrange(year, month, day)
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)), y = count)) +
geom_bar(stat = "identity", fill = "orange") +
labs(title = "Frequency of Articles Released by Year and Month",
x = "Year/Month",
y = "Number of Articles") +
theme(axis.text.x = element_text(size = 4, angle = 90, hjust = 1))
library('quanteda')
library('tm')
